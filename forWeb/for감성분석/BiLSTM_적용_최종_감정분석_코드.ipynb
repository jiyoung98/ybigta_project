{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BiLSTM ì ìš©-ìµœì¢… ê°ì •ë¶„ì„ ì½”ë“œ.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "8OQgWyj6hKdZ"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O6mNa9klXMyO",
        "outputId": "c3e893f1-b478-45b5-c6c6-062059fa2180"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1DwLIhLGiIv7",
        "outputId": "05351d7a-c4ca-421b-e0f8-101884fae12e"
      },
      "source": [
        "!pip install konlpy \n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import urllib.request\n",
        "from konlpy.tag import Okt\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: konlpy in /usr/local/lib/python3.7/dist-packages (0.5.2)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.7/dist-packages (from konlpy) (0.4.4)\n",
            "Requirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.19.5)\n",
            "Requirement already satisfied: beautifulsoup4==4.6.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.6.0)\n",
            "Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.2.1)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2gokgv_hK7l"
      },
      "source": [
        "tokenizer = Tokenizer(5542)"
      ],
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkdbvhFmhK-F"
      },
      "source": [
        "import json\n",
        "\n",
        "with open('/content/drive/MyDrive/Ybigta/á„‰á…µá†«á„‹á…µá†¸á„€á…µá„‰á…® á„‘á…³á„…á…©á„Œá…¦á†¨á„á…³/á„€á…¡á†·á„‰á…¥á†¼á„‡á…®á†«á„‰á…¥á†¨ á„†á…©á„ƒá…¦á†¯á„…á…µá†¼ á„á…®á„€á…¡(Bi-LSTM, Attention)/wordIndex(BiLSTM2).json') as json_file:\n",
        "  word_index = json.load(json_file)\n",
        "  tokenizer.word_index = word_index\n",
        "\n",
        "from keras.models import load_model\n",
        "model = load_model('/content/drive/MyDrive/Ybigta/á„‰á…µá†«á„‹á…µá†¸á„€á…µá„‰á…® á„‘á…³á„…á…©á„Œá…¦á†¨á„á…³/á„€á…¡á†·á„‰á…¥á†¼á„‡á…®á†«á„‰á…¥á†¨ á„†á…©á„ƒá…¦á†¯á„…á…µá†¼ á„á…®á„€á…¡(Bi-LSTM, Attention)/BiLSTM_model(sigmoid).h5')"
      ],
      "execution_count": 183,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfJfNA0CiDBQ"
      },
      "source": [
        "okt = Okt()"
      ],
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVgLwo9Vii38"
      },
      "source": [
        "max_len=30"
      ],
      "execution_count": 185,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CirbA4TrrnrH",
        "outputId": "fa41ffdb-715c-4242-8d35-b8981764b251"
      },
      "source": [
        "!pip install git+https://github.com/haven-jeon/PyKoSpacing.git\r\n",
        "\r\n",
        "from pykospacing import spacing\r\n"
      ],
      "execution_count": 186,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/haven-jeon/PyKoSpacing.git\n",
            "  Cloning https://github.com/haven-jeon/PyKoSpacing.git to /tmp/pip-req-build-idfsx8yc\n",
            "  Running command git clone -q https://github.com/haven-jeon/PyKoSpacing.git /tmp/pip-req-build-idfsx8yc\n",
            "Requirement already satisfied (use --upgrade to upgrade): pykospacing==0.4 from git+https://github.com/haven-jeon/PyKoSpacing.git in /usr/local/lib/python3.7/dist-packages\n",
            "Requirement already satisfied: tensorflow==2.4.0 in /usr/local/lib/python3.7/dist-packages (from pykospacing==0.4) (2.4.0)\n",
            "Requirement already satisfied: keras>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from pykospacing==0.4) (2.4.3)\n",
            "Requirement already satisfied: h5py==2.10.0 in /usr/local/lib/python3.7/dist-packages (from pykospacing==0.4) (2.10.0)\n",
            "Requirement already satisfied: argparse>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from pykospacing==0.4) (1.4.0)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0->pykospacing==0.4) (0.36.2)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0->pykospacing==0.4) (3.12.4)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0->pykospacing==0.4) (1.6.3)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0->pykospacing==0.4) (1.1.2)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0->pykospacing==0.4) (0.10.0)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0->pykospacing==0.4) (1.12.1)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0->pykospacing==0.4) (3.3.0)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0->pykospacing==0.4) (1.15.0)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0->pykospacing==0.4) (2.4.1)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0->pykospacing==0.4) (0.2.0)\n",
            "Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0->pykospacing==0.4) (1.32.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0->pykospacing==0.4) (0.3.3)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0->pykospacing==0.4) (1.19.5)\n",
            "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0->pykospacing==0.4) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0->pykospacing==0.4) (3.7.4.3)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0->pykospacing==0.4) (1.12)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0->pykospacing==0.4) (1.1.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras>=2.4.3->pykospacing==0.4) (3.13)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras>=2.4.3->pykospacing==0.4) (1.4.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.9.2->tensorflow==2.4.0->pykospacing==0.4) (53.0.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.0->pykospacing==0.4) (1.27.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.0->pykospacing==0.4) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.0->pykospacing==0.4) (1.8.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.0->pykospacing==0.4) (0.4.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.0->pykospacing==0.4) (2.23.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.0->pykospacing==0.4) (3.3.3)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.0->pykospacing==0.4) (4.7.1)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.0->pykospacing==0.4) (4.2.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.0->pykospacing==0.4) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow==2.4.0->pykospacing==0.4) (1.3.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.0->pykospacing==0.4) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.0->pykospacing==0.4) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.0->pykospacing==0.4) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.0->pykospacing==0.4) (2020.12.5)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow==2.4.0->pykospacing==0.4) (3.4.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.0->pykospacing==0.4) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow==2.4.0->pykospacing==0.4) (3.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow==2.4.0->pykospacing==0.4) (3.4.0)\n",
            "Building wheels for collected packages: pykospacing\n",
            "  Building wheel for pykospacing (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pykospacing: filename=pykospacing-0.4-cp37-none-any.whl size=2255638 sha256=f55221ca43a0e19f7d14e55f91d2a1dc6f3235524207db39e99e1cc44ed51725\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-i25p88ll/wheels/4d/45/58/e26cb2b7f6a063d234158c6fd1e5700f6e15b99d67154340ba\n",
            "Successfully built pykospacing\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4J67PWF2rnut",
        "outputId": "1404deae-54e8-43b0-f1de-7b33fc5f94f0"
      },
      "source": [
        "!pip install git+https://github.com/ssut/py-hanspell.git\r\n",
        "\r\n",
        "from hanspell import spell_checker\r\n",
        "def grammar_check(text):\r\n",
        "  spelled_sent = spell_checker.check(text)\r\n",
        "  hanspell_sent = spelled_sent.checked\r\n",
        "  return hanspell_sent"
      ],
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/ssut/py-hanspell.git\n",
            "  Cloning https://github.com/ssut/py-hanspell.git to /tmp/pip-req-build-pj52fpdp\n",
            "  Running command git clone -q https://github.com/ssut/py-hanspell.git /tmp/pip-req-build-pj52fpdp\n",
            "Requirement already satisfied (use --upgrade to upgrade): py-hanspell==1.1 from git+https://github.com/ssut/py-hanspell.git in /usr/local/lib/python3.7/dist-packages\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from py-hanspell==1.1) (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->py-hanspell==1.1) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->py-hanspell==1.1) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->py-hanspell==1.1) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->py-hanspell==1.1) (3.0.4)\n",
            "Building wheels for collected packages: py-hanspell\n",
            "  Building wheel for py-hanspell (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for py-hanspell: filename=py_hanspell-1.1-cp37-none-any.whl size=4854 sha256=ce631964e7e99f87c25e119c139330fb798449e77bbe51885be73aa62bb97c28\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-11on8je2/wheels/0a/25/d1/e5e96476dbb1c318cc26c992dd493394fe42b0c204b3e65588\n",
            "Successfully built py-hanspell\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZH8ABCEWrnwt"
      },
      "source": [
        "import re\r\n",
        "\r\n",
        "punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~\" + '\"\"â€œâ€â€™' + 'âˆÎ¸Ã·Î±â€¢Ã âˆ’Î²âˆ…Â³Ï€â€˜â‚¹Â´Â°Â£â‚¬\\Ã—â„¢âˆšÂ²â€”â€“&' \r\n",
        "punct_mapping = {\"â€˜\": \"'\", \"â‚¹\": \"e\", \"Â´\": \"'\", \"Â°\": \"\", \"â‚¬\": \"e\", \"â„¢\": \"tm\", \"âˆš\": \" sqrt \", \"Ã—\": \"x\", \"Â²\": \"2\",\r\n",
        "                 \"â€”\": \"-\", \"â€“\": \"-\", \"â€™\": \"'\", \"_\": \"-\", \"`\": \"'\", 'â€œ': '\"', 'â€': '\"', 'â€œ': '\"', \"Â£\": \"e\", 'âˆ': 'infinity', \r\n",
        "                 'Î¸': 'theta', 'Ã·': '/', 'Î±': 'alpha', 'â€¢': '.', 'Ã ': 'a', 'âˆ’': '-', 'Î²': 'beta', 'âˆ…': '', 'Â³': '3', 'Ï€': 'pi'} \r\n",
        "\r\n",
        "def clean_punc(text, punct, mapping): \r\n",
        "  for p in mapping: \r\n",
        "    text = text.replace(p, mapping[p]) \r\n",
        "  for p in punct: \r\n",
        "    text = text.replace(p, f' {p} ') \r\n",
        "    specials = {'\\u200b': ' ', 'â€¦': ' ... ', '\\ufeff': '', 'à¤•à¤°à¤¨à¤¾': '', 'à¤¹à¥ˆ': ''} \r\n",
        "  for s in specials: \r\n",
        "    text = text.replace(s, specials[s]) \r\n",
        "   \r\n",
        "  return text.strip() \r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "def clean_text(texts): \r\n",
        "  corpus = [] \r\n",
        "  for i in range(0, len(texts)): \r\n",
        "    review = re.sub(r'[@%\\\\*=()/~#&\\+Ã¡?\\xc3\\xa1\\-\\|\\.\\:\\;\\!\\-\\,\\_\\~\\$\\'\\\"]', '',str(texts[i])) #remove punctuation \r\n",
        "    review = re.sub(r'\\d+','', str(texts[i]))# remove number \r\n",
        "    review = review.lower() #lower case \r\n",
        "    review = re.sub(r'\\s+', ' ', review) #remove extra space \r\n",
        "    review = re.sub(r'<[^>]+>','',review) #remove Html tags \r\n",
        "    review = re.sub(r'\\s+', ' ', review) #remove spaces \r\n",
        "    review = re.sub(r\"^\\s+\", '', review) #remove space from start \r\n",
        "    review = re.sub(r'\\s+$', '', review) #remove space from the end \r\n",
        "    review = re.sub(r'â™¥','',review)\r\n",
        "    review = re.sub(r'~','',review)\r\n",
        "    review = re.sub(r'&','',review)\r\n",
        "    review = re.sub(r'ğŸ˜±','',review)\r\n",
        "    corpus.append(review) \r\n",
        "  return corpus\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# removing emoji\r\n",
        "\r\n",
        "def rmEmoji(text):\r\n",
        "  review = text.encode('utf-8', 'ignore').decode('utf-8')\r\n",
        "  return review\r\n",
        "\r\n",
        "def rmEmoji_ascii(inputString):\r\n",
        "  review = example.encode('ascii','ignore').decode('ascii')\r\n",
        "  return review\r\n"
      ],
      "execution_count": 188,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9djMfDyrq1h"
      },
      "source": [
        ""
      ],
      "execution_count": 188,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8c5QLeluqgBH",
        "outputId": "68602621-fd0e-4c5b-a284-63d613e1511c"
      },
      "source": [
        "import nltk\r\n",
        "from nltk.corpus import stopwords \r\n",
        "from nltk.tokenize import word_tokenize \r\n",
        "# NLTK Data ë‹¤ìš´\r\n",
        "nltk.download('punkt')\r\n",
        "nltk.download('wordnet')\r\n",
        "nltk.download('stopwords')\r\n",
        "\r\n",
        "stop_words = open('/content/drive/MyDrive/Ybigta/korean_stopwords.txt').read()\r\n",
        "\r\n",
        "\r\n",
        "# ìœ„ì˜ ë¶ˆìš©ì–´ëŠ” ëª…ì‚¬ê°€ ì•„ë‹Œ ë‹¨ì–´ ì¤‘ì—ì„œ ì €ìê°€ ì„ì˜ë¡œ ì„ ì •í•œ ê²ƒìœ¼ë¡œ ì‹¤ì œ ì˜ë¯¸ìˆëŠ” ì„ ì • ê¸°ì¤€ì´ ì•„ë‹˜\r\n",
        "stop_words=stop_words.split('\\n')"
      ],
      "execution_count": 189,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3pPRegdj2u0"
      },
      "source": [
        "def tokenize_tagged(text):\r\n",
        "\r\n",
        "  temp_X = okt.pos(text, norm=True, stem=True) # í† í°í™”\r\n",
        "  temp_X = [word for word in temp_X if not word in stop_words] # ë¶ˆìš©ì–´ ì œê±°\r\n",
        "  return ['/'.join(t) for t in temp_X]"
      ],
      "execution_count": 190,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83oAE8U3j2x0"
      },
      "source": [
        ""
      ],
      "execution_count": 190,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BrX6Olh_j27n",
        "outputId": "bb8544d7-31c7-4874-b60d-c283e5dc3aa8"
      },
      "source": [
        "tokenize_tagged(\"ì•ˆë…• ë§Œë‚˜ì„œ ë°˜ê°€ì›Œ\")"
      ],
      "execution_count": 191,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ì•ˆë…•/Noun', 'ë§Œë‚˜ë‹¤/Verb', 'ë°˜ê°‘ë‹¤/Adjective']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 191
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxniStEhiDDd"
      },
      "source": [
        "def sentiment_predict(new_sentence):\n",
        "  text1 = clean_punc(new_sentence, punct, punct_mapping)\n",
        "  text2 = ''.join(clean_text(text1))\n",
        "  text3 = spacing(text2)\n",
        "  text4 = grammar_check(text3)\n",
        "  text_ = tokenize_tagged(text4)\n",
        "\n",
        "  #new_sentence = okt.morphs(new_sentence, stem=True) # í† í°í™”\n",
        "  print(text_)\n",
        "  #new_sentence = [word for word in new_sentence if not word in stopwords] # ë¶ˆìš©ì–´ ì œê±°\n",
        "  #print(new_sentence)\n",
        "  encoded = tokenizer.texts_to_sequences([text_]) # ì •ìˆ˜ ì¸ì½”ë”©\n",
        "  print(encoded)\n",
        "  pad_new = pad_sequences(encoded, maxlen = max_len) # íŒ¨ë”©\n",
        "  print(pad_new)\n",
        "  score = float(model.predict(pad_new)) # ì˜ˆì¸¡\n",
        "  \n",
        "  print(\"{:.2f}% í™•ë¥ ë¡œ ê¸ì • ë¦¬ë·°ì…ë‹ˆë‹¤.\\n\".format(score * 100))\n",
        "  #if(score > 0.5):\n",
        "    #print(\"{:.2f}% í™•ë¥ ë¡œ ê¸ì • ë¦¬ë·°ì…ë‹ˆë‹¤.\\n\".format(score * 100))\n",
        "  #else:\n",
        "    #print(\"{:.2f}% í™•ë¥ ë¡œ ë¶€ì • ë¦¬ë·°ì…ë‹ˆë‹¤.\\n\".format((1 - score) * 100))"
      ],
      "execution_count": 192,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kKnLOSW6iDFe",
        "outputId": "e782ac33-942b-46a5-d1ae-b4ebe444ede9"
      },
      "source": [
        "sentiment_predict('ì—¬ê¸°ëŠ” ì»¤í”¼ê°€ ì •ë§ ë„ˆë¬´ ë§›ìˆì–´ìš” ìµœê³ ì˜ˆìš” ì§ì›ë„ ì¹œì ˆí•´ìš”')"
      ],
      "execution_count": 193,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['ì—¬ê¸°ë‹¤/Verb', 'ì»¤í”¼/Noun', 'ê°€/Josa', 'ì •ë§/Noun', 'ë„ˆë¬´/Adverb', 'ë§›ìˆë‹¤/Adjective', 'ìµœê³ /Noun', 'ì˜ˆ/Noun', 'ìš”/Noun', 'ì§ì›/Noun', 'ë„/Josa', 'ì¹œì ˆí•˜ë‹¤/Adjective']\n",
            "[[196, 7, 32, 8, 4, 10, 929, 36, 17, 18]]\n",
            "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0 196   7  32   8   4  10 929  36  17  18]]\n",
            "99.87% í™•ë¥ ë¡œ ê¸ì • ë¦¬ë·°ì…ë‹ˆë‹¤.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tARDQ3WZiDHj",
        "outputId": "edfe60ea-87c1-46ff-807d-11c68c653ad2"
      },
      "source": [
        "sentiment_predict('ë”ëŸ¬ì›Œ ì§ì›ì´ ë¶ˆì¹œì ˆí•´ìš” ì§œì¦ë‚˜ ì‹«ì–´')"
      ],
      "execution_count": 194,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['ë”ëŸ½ë‹¤/Adjective', 'ì§ì›/Noun', 'ì´/Josa', 'ë¶ˆì¹œì ˆí•˜ë‹¤/Adjective', 'ì§œì¦/Noun', 'ë‚˜/Noun', 'ì‹«ë‹¤/Adjective']\n",
            "[[398, 17, 67, 601, 109, 390]]\n",
            "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0 398  17  67 601 109 390]]\n",
            "2.41% í™•ë¥ ë¡œ ê¸ì • ë¦¬ë·°ì…ë‹ˆë‹¤.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RuNRvb6LhLCg",
        "outputId": "5ac84c81-aa91-4a7b-fa58-42610cfec249"
      },
      "source": [
        "sentiment_predict('ê·¸ëƒ¥ ì €ëƒ¥ ë³´í†µì´ì—ˆì–´ìš” ì˜¤ëŠ˜ í•˜ë£¨ì¢…ì¼ ê¸°ë¶„ì´ ìš°ìš¸í•´ìš”')"
      ],
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['ê·¸ëƒ¥/Modifier', 'ì €/Noun', 'ëƒ¥/Josa', 'ë³´í†µ/Noun', 'ì´ë‹¤/Verb', 'ì˜¤ëŠ˜/Noun', 'í•˜ë£¨/Noun', 'ì¢…ì¼/Noun', 'ê¸°ë¶„/Noun', 'ì´/Josa', 'ìš°ìš¸í•˜ë‹¤/Adjective']\n",
            "[[1014, 135, 367, 68, 317, 850, 1672, 98]]\n",
            "[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0 1014  135  367   68  317  850\n",
            "  1672   98]]\n",
            "45.76% í™•ë¥ ë¡œ ê¸ì • ë¦¬ë·°ì…ë‹ˆë‹¤.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eD-01ksahLEY",
        "outputId": "bba6a127-d5df-4a7f-bf30-7b67e8f476e2"
      },
      "source": [
        "sentiment_predict('ë§›ìˆë„¤ìš” ì˜ˆì˜ë„¤ìš” ë„“ì–´ìš”')"
      ],
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['ë§›ìˆë‹¤/Adjective', 'ì˜ˆì˜ë‹¤/Adjective', 'ë„“ë‹¤/Adjective']\n",
            "[[4, 63, 85]]\n",
            "[[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  4 63 85]]\n",
            "187.72% í™•ë¥ ë¡œ ê¸ì • ë¦¬ë·°ì…ë‹ˆë‹¤.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dw5lP5tlhLGi",
        "outputId": "4c51b29f-0cb2-482c-89fa-6345ef9dab6d"
      },
      "source": [
        "sentiment_predict('ê³µê°„ì´ ë„“ì–´ìš”')"
      ],
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['ê³µê°„/Noun', 'ì´/Josa', 'ë„“ë‹¤/Adjective']\n",
            "[[61, 85]]\n",
            "[[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0 61 85]]\n",
            "121.14% í™•ë¥ ë¡œ ê¸ì • ë¦¬ë·°ì…ë‹ˆë‹¤.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3bgEntMAhLI-",
        "outputId": "bdb7c056-7178-4356-de41-cbc5db4e8836"
      },
      "source": [
        "sentiment_predict('ì¸í…Œë¦¬ì–´ê°€ ì˜ë˜ì–´ìˆê³  ì¼€ì´í¬ ë§›ìˆìŒ')"
      ],
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['ì¸í…Œë¦¬ì–´/Noun', 'ê°€/Josa', 'ìë‹¤/Verb', 'ë˜ì–´ë‹¤/Verb', 'ìˆë‹¤/Adjective', 'ì¼€ì´í¬/Noun', 'ë§›ìˆë‹¤/Adjective']\n",
            "[[58, 46, 148, 6, 30, 4]]\n",
            "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0  58  46 148   6  30   4]]\n",
            "174.94% í™•ë¥ ë¡œ ê¸ì • ë¦¬ë·°ì…ë‹ˆë‹¤.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vMt2sbxShLLm",
        "outputId": "4b6e65d7-f0f9-4713-ad79-e8b5eaea18ff"
      },
      "source": [
        "sentiment_predict('ã…‹ã…‹ã…‹ë³„ë¡œ')"
      ],
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['ã…‹ã…‹ã…‹/KoreanParticle', 'ë³„ë¡œ/Noun']\n",
            "[[34]]\n",
            "[[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0 34]]\n",
            "6.40% í™•ë¥ ë¡œ ê¸ì • ë¦¬ë·°ì…ë‹ˆë‹¤.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLW3iVD2jQML",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1deb3343-2e79-4d99-e3a4-90089c7acf5e"
      },
      "source": [
        "sentiment_predict('ì• ë§¤í•´ìš”')"
      ],
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['ì• ë§¤í•˜ë‹¤/Adjective']\n",
            "[[1756]]\n",
            "[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0 1756]]\n",
            "98.54% í™•ë¥ ë¡œ ê¸ì • ë¦¬ë·°ì…ë‹ˆë‹¤.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "So8tq-polP4n"
      },
      "source": [
        ""
      ],
      "execution_count": 166,
      "outputs": []
    }
  ]
}