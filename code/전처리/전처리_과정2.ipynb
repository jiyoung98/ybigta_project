{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "μ „μ²λ¦¬ κ³Όμ •2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqH5lSvKMBx4"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "\r\n",
        "# tokenizer λ¶λ¬μ¤κΈ°\r\n",
        "from nltk.tokenize import word_tokenize, WordPunctTokenizer"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 876
        },
        "id": "l4xjdVjfMNE4",
        "outputId": "28b45d42-fa11-4027-f24a-6d1a084f66ec"
      },
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/α„‰α…µα†«α„‹α…µα†Έα„‘α…³α„…α…©α„α…¦α†¨α„α…³/traindata.csv')\r\n",
        "data.dropna(inplace = True)\r\n",
        "data.head(28)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>μΉ΄νλ…Έν‹°λ“ μ²­λ‹΄</td>\n",
              "      <td>λ„λ„› λ§›μ€ λ§›μμ λ€μ‹  ν¬μ¥ν•΄μ„ κ°κ±°λ©΄ μΈλ‚΄μ‹¬ κΈΈλ¬κ°€μ•Όν•¨ λ­”κ°€ ν¬μ¥ν•λ” λ°©μ‹μ΄ λ°”λ€...</td>\n",
              "      <td>3μ </td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>μΉ΄νλ…Έν‹°λ“ μ²­λ‹΄</td>\n",
              "      <td>κ³„μ‚°ν•΄μ£Όλ” μ§μ›λ„ λ¶μΉμ ν•κ³  ν¬λ΅ν”λ„ λ‘κ°μ‹μΌ°λ”λ° μ§‘μ— μ™€μ„ μ—΄μ–΄λ³΄λ‹ ν•κ°λ§ μ¤¬λ„¤μ”...</td>\n",
              "      <td>1μ </td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>μΉ΄νλ…Έν‹°λ“ μ²­λ‹΄</td>\n",
              "      <td>μΆ‹μ•„μ”</td>\n",
              "      <td>4μ </td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>μΉ΄νλ…Έν‹°λ“ μ²­λ‹΄</td>\n",
              "      <td>μµκ³ μμ”</td>\n",
              "      <td>5μ </td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>μΉ΄νλ…Έν‹°λ“ μ²­λ‹΄</td>\n",
              "      <td>λΉµλ“¤ λ‹¤ λ§›μκ³  μ €λ ΄ν•λ°λ‹¤ λ§¤μ¥ λ¶„μ„κΈ°λ„ μΆ‹μ€λ° μ§μ›λ“¤λΌλ¦¬ μΆ€ μ†ν†µμ΄ μ•λλ” λλ‚μ΄...</td>\n",
              "      <td>3μ </td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>μΉ΄νλ…Έν‹°λ“ μ²­λ‹΄</td>\n",
              "      <td>μµκ³ μμ”</td>\n",
              "      <td>5μ </td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6</td>\n",
              "      <td>μΉ΄νλ…Έν‹°λ“ μ²­λ‹΄</td>\n",
              "      <td>λ¨Ήκ³ μ‹¶μ—λ”λ° λ§μ΄ μ•λ‹¬μ•„μ„ μ§„μ§ λ” μΆ‹μ•λκ±°κ°™μ•„μ” μ¤„ μ •λ¦¬λ„ λΉ¨λΌμ„ μΆ‹μ•μµλ‹λ‹¤</td>\n",
              "      <td>5μ </td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>7</td>\n",
              "      <td>μΉ΄νλ…Έν‹°λ“ μ²­λ‹΄</td>\n",
              "      <td>μµκ³ μμ”</td>\n",
              "      <td>5μ </td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>8</td>\n",
              "      <td>μΉ΄νλ…Έν‹°λ“ μ²­λ‹΄</td>\n",
              "      <td>λ…Έν‹°λ“ λ§›μμ–΄μ” γ… γ… γ…</td>\n",
              "      <td>5μ </td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>9</td>\n",
              "      <td>μΉ΄νλ…Έν‹°λ“ μ²­λ‹΄</td>\n",
              "      <td>μ•„μ‰¬μ›μ”γ… γ… μ²μ λ…Έν‹°λ“ μ•κ² λκ³  λ¨Ήμ—μ„ λ•μ μ„¤λ κ³Ό κ°λ™μ΄ μ–΄λ μκ°„λ¶€ν„°λ” μ• λ...</td>\n",
              "      <td>3μ </td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>10</td>\n",
              "      <td>μΉ΄νλ…Έν‹°λ“ μ²­λ‹΄</td>\n",
              "      <td>μµκ³ μμ”</td>\n",
              "      <td>5μ </td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>11</td>\n",
              "      <td>μΉ΄νλ…Έν‹°λ“ μ²­λ‹΄</td>\n",
              "      <td>λλΌν•μ§€ μ•κ³  λ§‰ λ‹¬μ§€ μ•μ•„μ„ λ‚ λ§›μλλ…, κ°μ·¨μΈλ“―. μ»¤ν”Όλ„ λ¬΄λ‚ν•¨.</td>\n",
              "      <td>4μ </td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>12</td>\n",
              "      <td>μΉ΄νλ…Έν‹°λ“ μ²­λ‹΄</td>\n",
              "      <td>μµκ³ μμ”</td>\n",
              "      <td>5μ </td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>13</td>\n",
              "      <td>μΉ΄νλ…Έν‹°λ“ μ²­λ‹΄</td>\n",
              "      <td>λ‹Ή λ–¨μ–΄μ§ λ• κΌ­ κ°€λ³΄μ„Έμ”.</td>\n",
              "      <td>5μ </td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>14</td>\n",
              "      <td>μΉ΄νλ…Έν‹°λ“ μ²­λ‹΄</td>\n",
              "      <td>μµκ³ μμ”</td>\n",
              "      <td>5μ </td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>15</td>\n",
              "      <td>μΉ΄νλ…Έν‹°λ“ μ²­λ‹΄</td>\n",
              "      <td>λ…Έν‹°λ“λ” μ‚¬λ‘</td>\n",
              "      <td>4μ </td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>16</td>\n",
              "      <td>μΉ΄νλ…Έν‹°λ“ μ²­λ‹΄</td>\n",
              "      <td>μ¤„ κΈΈμ—λ”λ° λ§›μ€ κ± μμ</td>\n",
              "      <td>4μ </td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>17</td>\n",
              "      <td>μΉ΄νλ…Έν‹°λ“ μ²­λ‹΄</td>\n",
              "      <td>μµκ³ μμ”</td>\n",
              "      <td>5μ </td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>18</td>\n",
              "      <td>μΉ΄νλ…Έν‹°λ“ μ²­λ‹΄</td>\n",
              "      <td>λ‚®μ— μ‚¬μ„ λ°”λ΅ λ¨Ήμ„λ• λ§›λ‚¬λ”λ° μ €λ…μ— λ¨ΉμΌλ‹ λΉµμ΄ λ”±λ”±ν•΄μ§€κ³  κ°ν¥μ΄ μ—†λ„¤ κ·Έλ¦¬κ³  ...</td>\n",
              "      <td>3μ </td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>19</td>\n",
              "      <td>μΉ΄νλ…Έν‹°λ“ μ²­λ‹΄</td>\n",
              "      <td>λ§›μλ”λ° μ¤„ μ—„μ²­ κΈΈμ–΄μ§ μλ£λ” κ·Έλ‹¥</td>\n",
              "      <td>3μ </td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>20</td>\n",
              "      <td>μΉ΄νλ…Έν‹°λ“ μ²­λ‹΄</td>\n",
              "      <td>κ·€μ—½</td>\n",
              "      <td>5μ </td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>21</td>\n",
              "      <td>μΉ΄νλ…Έν‹°λ“ μ²­λ‹΄</td>\n",
              "      <td>γ…γ…γ…</td>\n",
              "      <td>4μ </td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>22</td>\n",
              "      <td>μΉ΄νλ…Έν‹°λ“ μ²­λ‹΄</td>\n",
              "      <td>μ»¤μ¤ν„°λ“ν¬λ¦Ό μ§„ν•΄μ”. λ…λ€λΉµμ€ λ³„λ΅. λΉµ μμ²΄κ°€ λ§›μμ§€λ” μ•μ..</td>\n",
              "      <td>4μ </td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>23</td>\n",
              "      <td>μΉ΄νλ…Έν‹°λ“ μ²­λ‹΄</td>\n",
              "      <td>λ§›μλ” λν‚¨λ„λ„› λ”± κ·Έ λ§μ΄ λ§λ‹¤</td>\n",
              "      <td>3μ </td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>24</td>\n",
              "      <td>μΉ΄νλ…Έν‹°λ“ μ²­λ‹΄</td>\n",
              "      <td>μΆ€ λ” λ§›μλ” λν‚¨λ„λ„› μ΄μ¦λ‹λ²„ν„°λ„λ„›: λ°λ°ν•λ° λ¬ν•κ² λ§›λ‚¨ μ¤λ§μΌμΌ€μ΄ν¬: μ΄‰μ΄‰ λ¶€...</td>\n",
              "      <td>4μ </td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>25</td>\n",
              "      <td>μΉ΄νλ…Έν‹°λ“ μ²­λ‹΄</td>\n",
              "      <td>λ§μ‹―κΈ΄ν•λ° λ‹¨κ±΄ λ„λ¬΄ λ‹΄ &amp; μ‚¬λμ΄ μ΅°μ¤μ¤μ¬λΌκ² λ§μ</td>\n",
              "      <td>3μ </td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>26</td>\n",
              "      <td>μΉ΄νλ…Έν‹°λ“ μ²­λ‹΄</td>\n",
              "      <td>μ°μ μƒν¬λ¦Όμ§± μΈν…λ¦¬μ–΄ μ΄μ‚¬κ°€κ³ λ‚μ„ κ°μ„±μ€ μ΅°κΈ λ–¨μ–΄μ΅μΌλ‚ μΆμ„ λ§μ•„μ Έμ„ κµΏ</td>\n",
              "      <td>5μ </td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>27</td>\n",
              "      <td>μΉ΄νλ…Έν‹°λ“ μ²­λ‹΄</td>\n",
              "      <td>κ¶κΈν•΄μ„ ν•λ²μ―¤ λ¨Ήμ–΄λ³Όλ§ν• γ…γ…</td>\n",
              "      <td>3μ </td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Unnamed: 0         0                                                  1   2\n",
              "0            0  μΉ΄νλ…Έν‹°λ“ μ²­λ‹΄  λ„λ„› λ§›μ€ λ§›μμ λ€μ‹  ν¬μ¥ν•΄μ„ κ°κ±°λ©΄ μΈλ‚΄μ‹¬ κΈΈλ¬κ°€μ•Όν•¨ λ­”κ°€ ν¬μ¥ν•λ” λ°©μ‹μ΄ λ°”λ€...  3μ \n",
              "1            1  μΉ΄νλ…Έν‹°λ“ μ²­λ‹΄  κ³„μ‚°ν•΄μ£Όλ” μ§μ›λ„ λ¶μΉμ ν•κ³  ν¬λ΅ν”λ„ λ‘κ°μ‹μΌ°λ”λ° μ§‘μ— μ™€μ„ μ—΄μ–΄λ³΄λ‹ ν•κ°λ§ μ¤¬λ„¤μ”...  1μ \n",
              "2            2  μΉ΄νλ…Έν‹°λ“ μ²­λ‹΄                                                μΆ‹μ•„μ”  4μ \n",
              "3            3  μΉ΄νλ…Έν‹°λ“ μ²­λ‹΄                                               μµκ³ μμ”  5μ \n",
              "4            4  μΉ΄νλ…Έν‹°λ“ μ²­λ‹΄  λΉµλ“¤ λ‹¤ λ§›μκ³  μ €λ ΄ν•λ°λ‹¤ λ§¤μ¥ λ¶„μ„κΈ°λ„ μΆ‹μ€λ° μ§μ›λ“¤λΌλ¦¬ μΆ€ μ†ν†µμ΄ μ•λλ” λλ‚μ΄...  3μ \n",
              "5            5  μΉ΄νλ…Έν‹°λ“ μ²­λ‹΄                                               μµκ³ μμ”  5μ \n",
              "6            6  μΉ΄νλ…Έν‹°λ“ μ²­λ‹΄        λ¨Ήκ³ μ‹¶μ—λ”λ° λ§μ΄ μ•λ‹¬μ•„μ„ μ§„μ§ λ” μΆ‹μ•λκ±°κ°™μ•„μ” μ¤„ μ •λ¦¬λ„ λΉ¨λΌμ„ μΆ‹μ•μµλ‹λ‹¤  5μ \n",
              "7            7  μΉ΄νλ…Έν‹°λ“ μ²­λ‹΄                                               μµκ³ μμ”  5μ \n",
              "8            8  μΉ΄νλ…Έν‹°λ“ μ²­λ‹΄                                       λ…Έν‹°λ“ λ§›μμ–΄μ” γ… γ… γ…  5μ \n",
              "9            9  μΉ΄νλ…Έν‹°λ“ μ²­λ‹΄  μ•„μ‰¬μ›μ”γ… γ… μ²μ λ…Έν‹°λ“ μ•κ² λκ³  λ¨Ήμ—μ„ λ•μ μ„¤λ κ³Ό κ°λ™μ΄ μ–΄λ μκ°„λ¶€ν„°λ” μ• λ...  3μ \n",
              "10          10  μΉ΄νλ…Έν‹°λ“ μ²­λ‹΄                                               μµκ³ μμ”  5μ \n",
              "11          11  μΉ΄νλ…Έν‹°λ“ μ²­λ‹΄           λλΌν•μ§€ μ•κ³  λ§‰ λ‹¬μ§€ μ•μ•„μ„ λ‚ λ§›μλλ…, κ°μ·¨μΈλ“―. μ»¤ν”Όλ„ λ¬΄λ‚ν•¨.   4μ \n",
              "12          12  μΉ΄νλ…Έν‹°λ“ μ²­λ‹΄                                               μµκ³ μμ”  5μ \n",
              "13          13  μΉ΄νλ…Έν‹°λ“ μ²­λ‹΄                                    λ‹Ή λ–¨μ–΄μ§ λ• κΌ­ κ°€λ³΄μ„Έμ”.  5μ \n",
              "14          14  μΉ΄νλ…Έν‹°λ“ μ²­λ‹΄                                               μµκ³ μμ”  5μ \n",
              "15          15  μΉ΄νλ…Έν‹°λ“ μ²­λ‹΄                                            λ…Έν‹°λ“λ” μ‚¬λ‘  4μ \n",
              "16          16  μΉ΄νλ…Έν‹°λ“ μ²­λ‹΄                                     μ¤„ κΈΈμ—λ”λ° λ§›μ€ κ± μμ  4μ \n",
              "17          17  μΉ΄νλ…Έν‹°λ“ μ²­λ‹΄                                               μµκ³ μμ”  5μ \n",
              "18          18  μΉ΄νλ…Έν‹°λ“ μ²­λ‹΄  λ‚®μ— μ‚¬μ„ λ°”λ΅ λ¨Ήμ„λ• λ§›λ‚¬λ”λ° μ €λ…μ— λ¨ΉμΌλ‹ λΉµμ΄ λ”±λ”±ν•΄μ§€κ³  κ°ν¥μ΄ μ—†λ„¤ κ·Έλ¦¬κ³  ...  3μ \n",
              "19          19  μΉ΄νλ…Έν‹°λ“ μ²­λ‹΄                               λ§›μλ”λ° μ¤„ μ—„μ²­ κΈΈμ–΄μ§ μλ£λ” κ·Έλ‹¥  3μ \n",
              "20          20  μΉ΄νλ…Έν‹°λ“ μ²­λ‹΄                                                 κ·€μ—½  5μ \n",
              "21          21  μΉ΄νλ…Έν‹°λ“ μ²­λ‹΄                                                γ…γ…γ…  4μ \n",
              "22          22  μΉ΄νλ…Έν‹°λ“ μ²­λ‹΄               μ»¤μ¤ν„°λ“ν¬λ¦Ό μ§„ν•΄μ”. λ…λ€λΉµμ€ λ³„λ΅. λΉµ μμ²΄κ°€ λ§›μμ§€λ” μ•μ..  4μ \n",
              "23          23  μΉ΄νλ…Έν‹°λ“ μ²­λ‹΄                                 λ§›μλ” λν‚¨λ„λ„› λ”± κ·Έ λ§μ΄ λ§λ‹¤  3μ \n",
              "24          24  μΉ΄νλ…Έν‹°λ“ μ²­λ‹΄  μΆ€ λ” λ§›μλ” λν‚¨λ„λ„› μ΄μ¦λ‹λ²„ν„°λ„λ„›: λ°λ°ν•λ° λ¬ν•κ² λ§›λ‚¨ μ¤λ§μΌμΌ€μ΄ν¬: μ΄‰μ΄‰ λ¶€...  4μ \n",
              "25          25  μΉ΄νλ…Έν‹°λ“ μ²­λ‹΄                      λ§μ‹―κΈ΄ν•λ° λ‹¨κ±΄ λ„λ¬΄ λ‹΄ & μ‚¬λμ΄ μ΅°μ¤μ¤μ¬λΌκ² λ§μ  3μ \n",
              "26          26  μΉ΄νλ…Έν‹°λ“ μ²­λ‹΄          μ°μ μƒν¬λ¦Όμ§± μΈν…λ¦¬μ–΄ μ΄μ‚¬κ°€κ³ λ‚μ„ κ°μ„±μ€ μ΅°κΈ λ–¨μ–΄μ΅μΌλ‚ μΆμ„ λ§μ•„μ Έμ„ κµΏ  5μ \n",
              "27          27  μΉ΄νλ…Έν‹°λ“ μ²­λ‹΄                                  κ¶κΈν•΄μ„ ν•λ²μ―¤ λ¨Ήμ–΄λ³Όλ§ν• γ…γ…  3μ "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hX-O8hriMaAv"
      },
      "source": [
        "import re\r\n",
        "\r\n",
        "punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~\" + '\"\"β€β€β€™' + 'βΞΈΓ·Ξ±β€ΆΓ β’Ξ²β…Β³Ο€β€β‚ΉΒ΄Β°Β£β‚¬\\Γ—β„ΆβΒ²β€”β€“&' \r\n",
        "punct_mapping = {\"β€\": \"'\", \"β‚Ή\": \"e\", \"Β΄\": \"'\", \"Β°\": \"\", \"β‚¬\": \"e\", \"β„Ά\": \"tm\", \"β\": \" sqrt \", \"Γ—\": \"x\", \"Β²\": \"2\",\r\n",
        "                 \"β€”\": \"-\", \"β€“\": \"-\", \"β€™\": \"'\", \"_\": \"-\", \"`\": \"'\", 'β€': '\"', 'β€': '\"', 'β€': '\"', \"Β£\": \"e\", 'β': 'infinity', \r\n",
        "                 'ΞΈ': 'theta', 'Γ·': '/', 'Ξ±': 'alpha', 'β€Ά': '.', 'Γ ': 'a', 'β’': '-', 'Ξ²': 'beta', 'β…': '', 'Β³': '3', 'Ο€': 'pi'} \r\n",
        "\r\n",
        "def clean_punc(text, punct, mapping): \r\n",
        "  for p in mapping: \r\n",
        "    text = text.replace(p, mapping[p]) \r\n",
        "  for p in punct: \r\n",
        "    text = text.replace(p, f' {p} ') \r\n",
        "    specials = {'\\u200b': ' ', 'β€¦': ' ... ', '\\ufeff': '', 'ΰ¤•ΰ¤°ΰ¤¨ΰ¤Ύ': '', 'ΰ¤Ήΰ¥': ''} \r\n",
        "  for s in specials: \r\n",
        "    text = text.replace(s, specials[s]) \r\n",
        "   \r\n",
        "  return text.strip() \r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "def clean_text(texts): \r\n",
        "  corpus = [] \r\n",
        "  for i in range(0, len(texts)): \r\n",
        "    review = re.sub(r'[@%\\\\*=()/~#&\\+Γ΅?\\xc3\\xa1\\-\\|\\.\\:\\;\\!\\-\\,\\_\\~\\$\\'\\\"]', '',str(texts[i])) #remove punctuation \r\n",
        "    review = re.sub(r'\\d+','', str(texts[i]))# remove number \r\n",
        "    review = review.lower() #lower case \r\n",
        "    review = re.sub(r'\\s+', ' ', review) #remove extra space \r\n",
        "    review = re.sub(r'<[^>]+>','',review) #remove Html tags \r\n",
        "    review = re.sub(r'\\s+', ' ', review) #remove spaces \r\n",
        "    review = re.sub(r\"^\\s+\", '', review) #remove space from start \r\n",
        "    review = re.sub(r'\\s+$', '', review) #remove space from the end \r\n",
        "    review = re.sub(r'β™¥','',review)\r\n",
        "    review = re.sub(r'~','',review)\r\n",
        "    review = re.sub(r'π±','',review)\r\n",
        "    corpus.append(review) \r\n",
        "  return corpus\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# removing emoji\r\n",
        "\r\n",
        "def rmEmoji(text):\r\n",
        "  review = text.encode('utf-8', 'ignore').decode('utf-8')\r\n",
        "  return review\r\n",
        "\r\n",
        "def rmEmoji_ascii(inputString):\r\n",
        "  review = example.encode('ascii','ignore').decode('ascii')\r\n",
        "  return review\r\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9xpurNkMm-j",
        "outputId": "fe46ee4b-6ab5-4c11-c87a-310426182581"
      },
      "source": [
        "!pip install git+https://github.com/ssut/py-hanspell.git\r\n",
        "\r\n",
        "from hanspell import spell_checker\r\n",
        "def grammar_check(text):\r\n",
        "  spelled_sent = spell_checker.check(text)\r\n",
        "  hanspell_sent = spelled_sent.checked\r\n",
        "  return hanspell_sent\r\n",
        "\r\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/ssut/py-hanspell.git\n",
            "  Cloning https://github.com/ssut/py-hanspell.git to /tmp/pip-req-build-4gnuxnfk\n",
            "  Running command git clone -q https://github.com/ssut/py-hanspell.git /tmp/pip-req-build-4gnuxnfk\n",
            "Requirement already satisfied (use --upgrade to upgrade): py-hanspell==1.1 from git+https://github.com/ssut/py-hanspell.git in /usr/local/lib/python3.6/dist-packages\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from py-hanspell==1.1) (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->py-hanspell==1.1) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->py-hanspell==1.1) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->py-hanspell==1.1) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->py-hanspell==1.1) (1.24.3)\n",
            "Building wheels for collected packages: py-hanspell\n",
            "  Building wheel for py-hanspell (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for py-hanspell: filename=py_hanspell-1.1-cp36-none-any.whl size=4854 sha256=c553e875cb179a83debab2fcd853650ba827288eff15733cc4eaae781bcc4347\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-bkjqbi72/wheels/0a/25/d1/e5e96476dbb1c318cc26c992dd493394fe42b0c204b3e65588\n",
            "Successfully built py-hanspell\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "wXlaF77BbOxG",
        "outputId": "078d9b44-2d54-47c5-fe4c-02b025f3e7d6"
      },
      "source": [
        "words = 'λ‹¤μ‹λ”κ±°κΈ°μ—κ°€μ§€μ•μ„κΊΌμ•Ό'\r\n",
        "checked = grammar_check(words)\r\n",
        "checked"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'λ‹¤μ‹λ” κ±°κΈ°μ— κ°€μ§€ μ•μ„ κ±°μ•Ό'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ha9qjGkyNxZX",
        "outputId": "cd15ec96-47aa-4c67-845f-8c91a96c6533"
      },
      "source": [
        "!pip install soynlp\r\n",
        "import urllib.request\r\n",
        "from soynlp import DoublespaceLineCorpus\r\n",
        "from soynlp.word import WordExtractor\r\n",
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/lovit/soynlp/master/tutorials/2016-10-20.txt\", filename=\"2016-10-20.txt\")\r\n",
        "corpus = DoublespaceLineCorpus(\"2016-10-20.txt\")\r\n",
        "word_extractor = WordExtractor()\r\n",
        "word_extractor.train(corpus)\r\n",
        "word_score_table = word_extractor.extract()\r\n",
        "\r\n",
        "from soynlp.tokenizer import LTokenizer\r\n",
        "scores = {word:score.cohesion_forward for word, score in word_score_table.items()}\r\n",
        "l_tokenizer = LTokenizer(scores=scores)\r\n",
        "\r\n",
        "def tokenizing(text):\r\n",
        "  return l_tokenizer.tokenize(text, flatten=False)\r\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: soynlp in /usr/local/lib/python3.6/dist-packages (0.0.493)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.6/dist-packages (from soynlp) (0.22.2.post1)\n",
            "Requirement already satisfied: numpy>=1.12.1 in /usr/local/lib/python3.6/dist-packages (from soynlp) (1.19.5)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from soynlp) (1.4.1)\n",
            "Requirement already satisfied: psutil>=5.0.1 in /usr/local/lib/python3.6/dist-packages (from soynlp) (5.4.8)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.20.0->soynlp) (1.0.0)\n",
            "training was done. used memory 1.145 Gb\n",
            "all cohesion probabilities was computed. # words = 223348\n",
            "all branching entropies was computed # words = 361598\n",
            "all accessor variety was computed # words = 361598\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "drnodLJTZmI4",
        "outputId": "efd6a0c7-70b5-4029-c215-998205be0a47"
      },
      "source": [
        "test_sample1 = data['1'][0]\r\n",
        "test_sample1"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'λ„λ„› λ§›μ€ λ§›μμ λ€μ‹  ν¬μ¥ν•΄μ„ κ°κ±°λ©΄ μΈλ‚΄μ‹¬ κΈΈλ¬κ°€μ•Όν•¨ λ­”κ°€ ν¬μ¥ν•λ” λ°©μ‹μ΄ λ°”λ€μ–΄μ•Ό ν• κ±°κ°™μ'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "PKX2kJROamLZ",
        "outputId": "1d59af55-215b-4e19-81cb-0106eb917d49"
      },
      "source": [
        "test_sample2 = clean_punc(test_sample1, punct, punct_mapping)\r\n",
        "test_sample2"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'λ„λ„› λ§›μ€ λ§›μμ λ€μ‹  ν¬μ¥ν•΄μ„ κ°κ±°λ©΄ μΈλ‚΄μ‹¬ κΈΈλ¬κ°€μ•Όν•¨ λ­”κ°€ ν¬μ¥ν•λ” λ°©μ‹μ΄ λ°”λ€μ–΄μ•Ό ν• κ±°κ°™μ'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "v6rDLcTTamHi",
        "outputId": "d71e38e1-531d-494c-8b8c-be2abd089b48"
      },
      "source": [
        "test_sample3 = ''.join(clean_text(test_sample2))\r\n",
        "test_sample3"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'λ„λ„›λ§›μ€λ§›μμλ€μ‹ ν¬μ¥ν•΄μ„κ°κ±°λ©΄μΈλ‚΄μ‹¬κΈΈλ¬κ°€μ•Όν•¨λ­”κ°€ν¬μ¥ν•λ”λ°©μ‹μ΄λ°”λ€μ–΄μ•Όν• κ±°κ°™μ'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gefQ4fEqddai",
        "outputId": "8f797a40-7f81-4429-e97f-a6a57bab6855"
      },
      "source": [
        "!pip install git+https://github.com/haven-jeon/PyKoSpacing.git\r\n",
        "\r\n",
        "from pykospacing import spacing\r\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/haven-jeon/PyKoSpacing.git\n",
            "  Cloning https://github.com/haven-jeon/PyKoSpacing.git to /tmp/pip-req-build-nppv_or1\n",
            "  Running command git clone -q https://github.com/haven-jeon/PyKoSpacing.git /tmp/pip-req-build-nppv_or1\n",
            "Requirement already satisfied (use --upgrade to upgrade): pykospacing==0.4 from git+https://github.com/haven-jeon/PyKoSpacing.git in /usr/local/lib/python3.6/dist-packages\n",
            "Requirement already satisfied: tensorflow==2.4.0 in /usr/local/lib/python3.6/dist-packages (from pykospacing==0.4) (2.4.0)\n",
            "Requirement already satisfied: keras>=2.4.3 in /usr/local/lib/python3.6/dist-packages (from pykospacing==0.4) (2.4.3)\n",
            "Requirement already satisfied: h5py==2.10.0 in /usr/local/lib/python3.6/dist-packages (from pykospacing==0.4) (2.10.0)\n",
            "Requirement already satisfied: argparse>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from pykospacing==0.4) (1.4.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.4.0->pykospacing==0.4) (1.12)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.4.0->pykospacing==0.4) (0.10.0)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.4.0->pykospacing==0.4) (3.7.4.3)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.4.0->pykospacing==0.4) (0.3.3)\n",
            "Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.4.0->pykospacing==0.4) (1.32.0)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.4.0->pykospacing==0.4) (2.4.1)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.4.0->pykospacing==0.4) (1.6.3)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.4.0->pykospacing==0.4) (1.1.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.4.0->pykospacing==0.4) (2.4.0)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.4.0->pykospacing==0.4) (1.15.0)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.4.0->pykospacing==0.4) (1.19.5)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.4.0->pykospacing==0.4) (1.1.2)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.4.0->pykospacing==0.4) (1.12.1)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.4.0->pykospacing==0.4) (3.3.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.4.0->pykospacing==0.4) (3.12.4)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.4.0->pykospacing==0.4) (0.36.2)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.4.0->pykospacing==0.4) (0.2.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras>=2.4.3->pykospacing==0.4) (1.4.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras>=2.4.3->pykospacing==0.4) (3.13)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow==2.4.0->pykospacing==0.4) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow==2.4.0->pykospacing==0.4) (3.3.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow==2.4.0->pykospacing==0.4) (2.23.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow==2.4.0->pykospacing==0.4) (53.0.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow==2.4.0->pykospacing==0.4) (0.4.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow==2.4.0->pykospacing==0.4) (1.8.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow==2.4.0->pykospacing==0.4) (1.25.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow==2.4.0->pykospacing==0.4) (3.4.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.0->pykospacing==0.4) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.0->pykospacing==0.4) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.0->pykospacing==0.4) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.0->pykospacing==0.4) (3.0.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow==2.4.0->pykospacing==0.4) (1.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.0->pykospacing==0.4) (4.7)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.0->pykospacing==0.4) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.0->pykospacing==0.4) (4.2.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow==2.4.0->pykospacing==0.4) (3.4.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow==2.4.0->pykospacing==0.4) (3.1.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.0->pykospacing==0.4) (0.4.8)\n",
            "Building wheels for collected packages: pykospacing\n",
            "  Building wheel for pykospacing (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pykospacing: filename=pykospacing-0.4-cp36-none-any.whl size=2255638 sha256=aae5737c68ce03e1351e4325fc1c8dcf2707125bb6f428d74850a917ad0517b9\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-41lyqwg9/wheels/4d/45/58/e26cb2b7f6a063d234158c6fd1e5700f6e15b99d67154340ba\n",
            "Successfully built pykospacing\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "pGqZqGi-amEb",
        "outputId": "aefc4e6e-b650-45d5-b4c9-c7b2698f5677"
      },
      "source": [
        "test_sample5 = grammar_check(test_sample4)\r\n",
        "test_sample5"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'λ„λ„› λ§›μ€ λ§›μμ λ€μ‹  ν¬μ¥ν•΄μ„ κ° κ±°λ©΄ μΈλ‚΄μ‹¬ κΈΈλ¬ κ°€μ•Ό ν•¨ λ­”κ°€ ν¬μ¥ν•λ” λ°©μ‹μ΄ λ°”λ€μ–΄μ•Ό ν•  κ±° κ°™μ'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iblR3bFfal_G",
        "outputId": "f2bffb8d-65a7-4da7-eee2-ecc09da249bc"
      },
      "source": [
        "test_sample6 = tokenizing(test_sample5)\r\n",
        "test_sample6"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('λ„λ„›', ''),\n",
              " ('λ§›μ€', ''),\n",
              " ('λ§›μ', 'μ'),\n",
              " ('λ€μ‹ ', ''),\n",
              " ('ν¬μ¥ν•΄μ„', ''),\n",
              " ('κ°', ''),\n",
              " ('κ±°λ©΄', ''),\n",
              " ('μΈλ‚΄μ‹¬', ''),\n",
              " ('κΈΈλ¬', ''),\n",
              " ('κ°€μ•Ό', ''),\n",
              " ('ν•¨', ''),\n",
              " ('λ­”κ°€', ''),\n",
              " ('ν¬μ¥ν•λ”', ''),\n",
              " ('λ°©μ‹μ΄', ''),\n",
              " ('λ°”λ€μ–΄μ•Ό', ''),\n",
              " ('ν• ', ''),\n",
              " ('κ±°', ''),\n",
              " ('κ°™μ', '')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sr_xayGHOx0q"
      },
      "source": [
        "# normalizerλ” μ•„μ§ λ―Έμ™„\r\n",
        "from soynlp.normalizer import *\r\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZPcf4a5O8IC",
        "outputId": "6d4c27c7-c490-4840-ad66-a945f0c3e2c5"
      },
      "source": [
        "!pip install konlpy \r\n",
        "from konlpy.tag import Okt\r\n",
        "okt = Okt()\r\n",
        "\r\n",
        "def tokenize_tagged(text):\r\n",
        "  #ν•νƒμ†μ™€ ν’μ‚¬λ¥Ό join\r\n",
        "\r\n",
        "  temp_X = okt.pos(text, norm=True, stem=True) # ν† ν°ν™”\r\n",
        "  temp_X = [word for word in temp_X if not word in stop_words] # λ¶μ©μ–΄ μ κ±°\r\n",
        "  return ['/'.join(t) for t in temp_X]\r\n",
        "  \r\n",
        "  #return ['/'.join(t) for t in okt.pos(text, norm=True, stem=True)]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: konlpy in /usr/local/lib/python3.6/dist-packages (0.5.2)\n",
            "Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (1.2.1)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.6/dist-packages (from konlpy) (0.4.4)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (4.2.6)\n",
            "Requirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.6/dist-packages (from konlpy) (1.19.5)\n",
            "Requirement already satisfied: beautifulsoup4==4.6.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (4.6.0)\n",
            "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPa-yQZ0hHUc",
        "outputId": "eaf3e037-82b1-41c9-9020-5b440d3fd573"
      },
      "source": [
        "# stemming\r\n",
        "!pip install soylemma==0.2.0\r\n",
        "from soylemma import Lemmatizer\r\n",
        "\r\n",
        "lemmatizer = Lemmatizer()\r\n",
        "\r\n",
        "# or\r\n",
        "lemmatizer = Lemmatizer(dictionary_name='default')\r\n",
        "\r\n",
        "text = 'λ‹¬λ Έλ”λ°'\r\n",
        "print(lemmatizer.lemmatize(text))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting soylemma==0.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/03/f5/d612c52b363a1e04b2df6dbde86bedd84035ba3d07651c586dcfdac50b53/soylemma-0.2.0-py3-none-any.whl (124kB)\n",
            "\r\u001b[K     |β–β–β–‹                             | 10kB 17.5MB/s eta 0:00:01\r\u001b[K     |β–β–β–β–β–β–                          | 20kB 22.0MB/s eta 0:00:01\r\u001b[K     |β–β–β–β–β–β–β–β–                        | 30kB 10.9MB/s eta 0:00:01\r\u001b[K     |β–β–β–β–β–β–β–β–β–β–β–‹                     | 40kB 8.7MB/s eta 0:00:01\r\u001b[K     |β–β–β–β–β–β–β–β–β–β–β–β–β–β–                  | 51kB 7.1MB/s eta 0:00:01\r\u001b[K     |β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–‰                | 61kB 6.9MB/s eta 0:00:01\r\u001b[K     |β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–             | 71kB 7.2MB/s eta 0:00:01\r\u001b[K     |β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–          | 81kB 8.0MB/s eta 0:00:01\r\u001b[K     |β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–        | 92kB 7.8MB/s eta 0:00:01\r\u001b[K     |β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–     | 102kB 8.1MB/s eta 0:00:01\r\u001b[K     |β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–   | 112kB 8.1MB/s eta 0:00:01\r\u001b[K     |β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–| 122kB 8.1MB/s eta 0:00:01\r\u001b[K     |β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–| 133kB 8.1MB/s \n",
            "\u001b[?25hInstalling collected packages: soylemma\n",
            "Successfully installed soylemma-0.2.0\n",
            "[('λ‹¬λ¦¬λ‹¤', 'Verb')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-4rmITWk24C",
        "outputId": "a256b7b1-92e4-41da-cf6f-51fdb944f7fe"
      },
      "source": [
        "text = 'λ‹¬λ¦¬λ‹¤'\r\n",
        "lemmatizer.lemmatize(text)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KomjWlTjQCtt",
        "outputId": "2aa0dfcc-bc26-4c0d-83ef-713191cb9035"
      },
      "source": [
        "#main()\r\n",
        "token_list = []\r\n",
        "i = 0\r\n",
        "for text in data['1']:\r\n",
        "  i += 1\r\n",
        "  print(str(i) + 'λ²μ§Έ λ¦¬λ·°!')\r\n",
        "  text1 = clean_punc(text, punct, punct_mapping)\r\n",
        "  text2 = ''.join(clean_text(text1))\r\n",
        "  text3 = spacing(text2)\r\n",
        "  text4 = grammar_check(text3)\r\n",
        "  text5 = tokenizing(text4)\r\n",
        "  text_ = tokenize_tagged(text4)\r\n",
        "  token_list.append(text_)\r\n",
        "\r\n",
        "  print(text_)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1λ²μ§Έ λ¦¬λ·°!\n",
            "['λ„λ„›/Noun', 'λ§›/Noun', 'μ€/Josa', 'λ§›μλ‹¤/Adjective', 'λ€μ‹ /Noun', 'ν¬μ¥/Noun', 'ν•λ‹¤/Verb', 'κ°λ‹¤/Verb', 'κ±°/Noun', 'λ©΄/Josa', 'μΈλ‚΄μ‹¬/Noun', 'κΈ°λ¥΄λ‹¤/Verb', 'κ°€μ•Ό/Noun', 'ν•¨/Noun', 'λ­”κ°€/Noun', 'ν¬μ¥/Noun', 'ν•λ‹¤/Verb', 'λ°©μ‹/Noun', 'μ΄/Josa', 'λ°”λ€λ‹¤/Verb', 'ν•λ‹¤/Verb', 'κ±°/Noun', 'κ°™λ‹¤/Adjective']\n",
            "2λ²μ§Έ λ¦¬λ·°!\n",
            "['κ³„μ‚°/Noun', 'ν•΄μ£Όλ‹¤/Verb', 'μ§μ›/Noun', 'λ„/Josa', 'λ¶μΉμ ν•λ‹¤/Adjective', 'ν¬λ‹¤/Verb', 'ν”„λ΅λ„/Noun', 'λ‘/Noun', 'κ°μ‹/Noun', 'μΌλ‹¤/Verb', 'μ§‘/Noun', 'μ—/Josa', 'μ¤λ‹¤/Verb', 'μ—΄/Modifier', 'μ–΄λ³΄/Noun', 'λ‹/Josa', 'ν•λ‹¤/Verb', 'κ°/Noun', 'λ§/Josa', 'μ£Όλ‹¤/Verb', '...../Punctuation', 'λ‹¤μ‹λ‹¤/Verb', 'μ•/Noun', 'κ°λ‹¤/Verb', 'μμ •/Noun', 'μ΄λ‹¤/Adjective', '../Punctuation']\n",
            "3λ²μ§Έ λ¦¬λ·°!\n",
            "['μΆ‹λ‹¤/Adjective']\n",
            "4λ²μ§Έ λ¦¬λ·°!\n",
            "['μµκ³ /Noun', 'μμ”/Josa']\n",
            "5λ²μ§Έ λ¦¬λ·°!\n",
            "['λΉµ/Noun', 'λ“¤/Suffix', 'λ”/Noun', 'λ§›μλ‹¤/Adjective', 'μ €λ ΄ν•λ‹¤/Adjective', 'λ°/Noun', 'λ‹¤/Josa', 'λ§¤μ¥/Noun', 'λ¶„μ„κΈ°/Noun', 'λ„/Josa', 'μΆ‹λ‹¤/Adjective', 'μ§μ›/Noun', 'λ“¤/Suffix', 'λΌλ¦¬/Noun', 'μΆ€/Noun', 'μ†ν†µ/Noun', 'μ΄/Josa', 'μ•/Noun', 'λλ‹¤/Verb', 'λλ‚/Noun', 'μ΄λΌ/Josa', 'μ•„μ‰½λ‹¤/Adjective', 'κ³„μ‚°/Noun', 'ν•λ‹¤/Verb', 'λ•/Noun', 'λΉµ/Noun', 'μ€/Josa', 'μ–΄λ””μ„/Adverb', 'λ°›λ‹¤/Verb', 'μΌ€μ΄ν¬/Noun', 'λ”/Josa', 'μ–΄λ””μ„/Adverb', 'λ°›λ‹¤/Verb', 'λ―Έλ¦¬/Noun', 'λ§μ”€/Noun', 'ν•΄μ£Όλ‹¤/Verb', '../Punctuation']\n",
            "6λ²μ§Έ λ¦¬λ·°!\n",
            "['μµκ³ /Noun', 'μμ”/Josa']\n",
            "7λ²μ§Έ λ¦¬λ·°!\n",
            "['λ¨Ήλ‹¤/Verb', 'μ‹¶λ‹¤/Verb', 'λ§μ΄/Adverb', 'μ•/Noun', 'λ‹¬λ‹¤/Verb', 'μ§„μ§/Noun', 'λ”/Noun', 'μΆ‹λ‹¤/Adjective', 'κ±°/Noun', 'κ°™λ‹¤/Adjective', 'μ¤„/Noun', 'μ •λ¦¬/Noun', 'λ„/Josa', 'λΉ¨/Noun', 'λΌμ„/Josa', 'μΆ‹λ‹¤/Adjective']\n",
            "8λ²μ§Έ λ¦¬λ·°!\n",
            "['μµκ³ /Noun', 'μμ”/Josa']\n",
            "9λ²μ§Έ λ¦¬λ·°!\n",
            "['λ…Έ/Noun', 'ν‹°μ¤/Noun', 'λ§›μλ‹¤/Adjective', 'γ… γ… γ…/KoreanParticle']\n",
            "10λ²μ§Έ λ¦¬λ·°!\n",
            "['μ•„μ‰½λ‹¤/Adjective', 'γ… γ… /KoreanParticle', 'μ²μ/Noun', 'λ…Έ/Noun', 'ν‹°μ¤/Noun', 'μ•/Noun', 'κ²/Josa', 'λλ‹¤/Verb', 'λ¨Ήλ‹¤/Verb', 'λ•/Noun', 'μ/Josa', 'μ„¤λ λ‹¤/Adjective', 'κ°λ™/Noun', 'μ΄/Josa', 'μ–΄λ/Adverb', 'μκ°„/Noun', 'λ¶€ν„°λ”/Josa', 'μ•/Noun', 'λκ»΄μ§€λ‹¤/Verb', './Punctuation', 'ν•λ‹¤/Verb', 'μ…/Noun', 'λ² λ‹¤/Verb', 'λ¬Ό/Noun', 'κ³ /Josa', 'λ/Noun', 'μ΄/Josa', 'λ¨μ΄λ‹¤/Verb', 'μ¤λ‹¤/Verb', 'μ§„μ§/Noun', 'λΉµ/Noun', 'μ΄/Josa', 'μ΄λ ‡κ²/Adverb', 'λ‹¤λ¥΄λ‹¤/Adjective', 'μ/Noun', 'μλ‹¤/Adjective', 'ν•λ‹¤/Verb', 'κ·Έλ•/Noun', 'λ/Noun', 'μ΄/Josa', 'μ—†λ‹¤/Adjective', 'ν•λ‘/Modifier', 'κ°/Noun', 'λ°–μ—/Josa', 'λ»/Noun', 'λ¨Ήλ‹¤/Verb', 'μ§€κΈ/Noun', 'μ€/Josa', 'λ§μ΄/Adverb', 'μ‚΄/Noun', 'μ/Noun', 'μλ‹¤/Adjective', 'κ°λ™/Noun', 'μ/Josa', 'λ§›/Noun', 'μ΄/Josa', 'μ—†λ‹¤/Adjective', 'γ…γ…/KoreanParticle', 'κ·Έλ ‡κ²/Adverb', 'λλ‹¤/Verb', 'μ§€λ‹¤/Verb', 'λ‡/Noun', 'λ²/Noun', 'λΌλ‹¤/Verb', 'νΉμ‹/Noun', 'λ‚/Josa', 'νΉμ‹/Noun', 'λ‚/Josa', 'ν•λ‹¤/Verb', 'κΈ°/Modifier', 'λ€κ°/Noun', 'μ—/Josa', ',/Punctuation', 'μ²μ/Noun', 'κ·Έ/Noun', 'λ§›/Noun', 'μ„/Josa', 'μλ‹¤/Verb', 'μκ°€/Noun', 'μ—†λ‹¤/Adjective', 'λ/Noun', 'μ‚¬/Noun', 'λ¨Ήλ‹¤/Verb', '.../Punctuation']\n",
            "11λ²μ§Έ λ¦¬λ·°!\n",
            "['μµκ³ /Noun', 'μμ”/Josa']\n",
            "12λ²μ§Έ λ¦¬λ·°!\n",
            "['λλΌν•λ‹¤/Adjective', 'μ•λ‹¤/Verb', 'λ§‰/Noun', 'λ‹¬λ¦¬/Noun', 'μ•λ‹¤/Verb', 'λ‚/Noun', 'λ§›μλ‹¤/Adjective', 'λ…/Noun', ',/Punctuation', 'κ°μ·¨/Noun', 'μΈ/Josa', 'λ“―/Noun', './Punctuation', 'μ»¤ν”Ό/Noun', 'λ„/Josa', 'λ¬΄λ‚/Noun', 'ν•¨/Noun', './Punctuation']\n",
            "13λ²μ§Έ λ¦¬λ·°!\n",
            "['μµκ³ /Noun', 'μμ”/Josa']\n",
            "14λ²μ§Έ λ¦¬λ·°!\n",
            "['λ‹Ή/Noun', 'λ–¨μ–΄μ§€λ‹¤/Verb', 'λ•/Noun', 'κΌ­/Noun', 'κ°€λ³΄λ‹¤/Verb', './Punctuation']\n",
            "15λ²μ§Έ λ¦¬λ·°!\n",
            "['μµκ³ /Noun', 'μμ”/Josa']\n",
            "16λ²μ§Έ λ¦¬λ·°!\n",
            "['λ…Έν‹°/Noun', 'λ“¤λ‹¤/Verb', 'μ‚¬λ‘/Noun']\n",
            "17λ²μ§Έ λ¦¬λ·°!\n",
            "['μ¤„/Noun', 'κΈΈλ‹¤/Adjective', 'λ§›/Noun', 'μ€/Josa', 'κ±/Adverb', 'μλ‹¤/Verb', 'μλ‹¤/Verb']\n",
            "18λ²μ§Έ λ¦¬λ·°!\n",
            "['μµκ³ /Noun', 'μμ”/Josa']\n",
            "19λ²μ§Έ λ¦¬λ·°!\n",
            "['λ‚®/Noun', 'μ—/Josa', 'μ‚¬μ„/Noun', 'λ°”λ΅/Noun', 'λ¨Ήλ‹¤/Verb', 'λ•/Noun', 'λ§›/Noun', 'λ‚λ‹¤/Verb', 'μ €λ…/Noun', 'μ—/Josa', 'λ¨Ήλ‹¤/Verb', 'λΉµ/Noun', 'μ΄/Josa', 'λ”±λ”±ν•λ‹¤/Adjective', 'κ°ν¥/Noun', 'μ΄/Josa', 'μ—†λ‹¤/Adjective', 'κ·Έλ¦¬κ³ /Conjunction', 'μΉ΄μ•Ό/Noun', './Punctuation', 'μ•™/Adverb', 'λ²„ν„°/Noun', 'λ²Όλ΅/Noun', 'μ΄λ‹¤/Adjective', 'λ””λ‹¤/Noun', './Punctuation', 'λΉ„μ¶”λ‹¤/Verb', '!/Punctuation']\n",
            "20λ²μ§Έ λ¦¬λ·°!\n",
            "['λ§›μλ‹¤/Adjective', 'μ¤„/Noun', 'μ—„μ²­/Adverb', 'κΈΈ/Noun', 'μ–΄μ§/Noun', 'μλ£/Noun', 'λ”/Josa', 'κ·Έλ‹¤μ§€/Noun']\n",
            "21λ²μ§Έ λ¦¬λ·°!\n",
            "['κ·€μ—½λ‹¤/Adjective']\n",
            "22λ²μ§Έ λ¦¬λ·°!\n",
            "['γ…γ…γ…/KoreanParticle']\n",
            "23λ²μ§Έ λ¦¬λ·°!\n",
            "['μ»¤μ¤ν„°λ“/Noun', 'ν¬λ¦Ό/Noun', 'μ§„ν•΄/Noun', 'μ”/Josa', './Punctuation', 'λ…λ€/Noun', 'λΉµ/Noun', 'μ€/Josa', 'λ³„λ΅/Noun', './Punctuation', 'λΉµ/Noun', 'μμ²΄/Noun', 'κ°€/Josa', 'λ§›μλ‹¤/Adjective', 'μ•λ‹¤/Verb', '../Punctuation']\n",
            "24λ²μ§Έ λ¦¬λ·°!\n",
            "['λ§›μλ‹¤/Adjective', 'λν‚¨/Noun', 'λ„λ„›/Noun', 'λ”±/Adverb', 'κ·Έ/Noun', 'λ§/Noun', 'μ΄/Josa', 'λ§λ‹¤/Verb']\n",
            "25λ²μ§Έ λ¦¬λ·°!\n",
            "['μΆ€/Noun', 'λ”/Noun', 'λ§›μλ‹¤/Adjective', 'λν‚¨/Noun', 'λ„λ„›/Noun', 'μ΄μ¦/Noun', 'λ‹/Josa', 'λ²„ν„°/Noun', 'λ„λ„›/Noun', ':/Punctuation', 'λ°λ°/Noun', 'ν•/Determiner', 'λ°/Noun', 'λ¬/Noun', 'ν•λ‹¤/Verb', 'λ§›/Noun', 'λ‚¨/Noun', 'μ¤λ§μΌ/Noun', 'μΌ€μ΄ν¬/Noun', ':/Punctuation', 'μ΄‰μ΄‰/Noun', 'λ¶€λ“λ½λ‹¤/Adjective', 'λ§›/Noun', 'λ‚¨/Noun', 'ν† μ”μΌ/Noun', 'μ‹/Noun', 'κΈ°μ¤€/Noun', ',/Punctuation', 'λ¶„/Noun', 'μ΄/Josa', 'μƒ/Noun', 'λ€κΈ°/Noun']\n",
            "26λ²μ§Έ λ¦¬λ·°!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ParseError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<string>\"\u001b[0;36m, line \u001b[0;32munknown\u001b[0m\n\u001b[0;31mParseError\u001b[0m\u001b[0;31m:\u001b[0m not well-formed (invalid token): line 1, column 55\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xahiZmvrfjIl",
        "outputId": "19a6edaf-9d4b-41ee-e412-b57647121935"
      },
      "source": [
        "import nltk\r\n",
        "from nltk.corpus import stopwords \r\n",
        "from nltk.tokenize import word_tokenize \r\n",
        "# NLTK Data λ‹¤μ΄\r\n",
        "nltk.download('punkt')\r\n",
        "nltk.download('wordnet')\r\n",
        "nltk.download('stopwords')\r\n",
        "\r\n",
        "stop_words = open('/content/drive/MyDrive/α„‰α…µα†«α„‹α…µα†Έα„‘α…³α„…α…©α„α…¦α†¨α„α…³/korean_stopwords.txt').read()\r\n",
        "\r\n",
        "\r\n",
        "# μ„μ λ¶μ©μ–΄λ” λ…μ‚¬κ°€ μ•„λ‹ λ‹¨μ–΄ μ¤‘μ—μ„ μ €μκ°€ μ„μλ΅ μ„ μ •ν• κ²ƒμΌλ΅ μ‹¤μ  μλ―Έμλ” μ„ μ • κΈ°μ¤€μ΄ μ•„λ‹\r\n",
        "stop_words=stop_words.split('\\n')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pT-dE0Z-3vN1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}